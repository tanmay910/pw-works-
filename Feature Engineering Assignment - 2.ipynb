{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe32ad8f",
   "metadata": {},
   "source": [
    "# Feature Engineering Assignment - 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0a9d4",
   "metadata": {},
   "source": [
    "Certainly! Here are the answers to your questions:\n",
    "\n",
    "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique that transforms the features of a dataset to a specific range, usually between 0 and 1. It is used to ensure that features with different scales have similar ranges, which can improve the performance of machine learning algorithms that are sensitive to feature scales.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with a feature representing income in dollars ranging from $20,000 to $200,000. Applying Min-Max scaling would transform these values to a range between 0 and 1, making them more suitable for processing by machine learning algorithms.\n",
    "\n",
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
    "\n",
    "The Unit Vector technique, also known as vector normalization, scales the features of a dataset such that each data point forms a unit vector (i.e., has a Euclidean norm of 1). Unlike Min-Max scaling, which focuses on transforming features within a specific range, Unit Vector scaling ensures that the data points lie on the surface of a hypersphere with a radius of 1.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with two features: [3, 4]. After applying Unit Vector scaling, the feature vector [3, 4] is divided by its Euclidean norm (5), resulting in the unit vector [0.6, 0.8].\n",
    "\n",
    "**Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining as much variance as possible. It achieves this by finding the principal components, which are orthogonal directions along which the data varies the most.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset with features [height, weight, age], and you apply PCA. The first principal component might represent a combination of height and weight, capturing the most significant variation in the data, while the second principal component might represent age. By choosing to retain only the first principal component, you reduce the dimensionality of the data while preserving a substantial portion of its variance.\n",
    "\n",
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
    "\n",
    "PCA and Feature Extraction are related in that PCA can be used as a feature extraction technique. PCA identifies the most important dimensions (principal components) in the data and projects the original features onto these dimensions, effectively extracting new features that capture the most significant variability.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with multiple correlated features. By applying PCA, you obtain the principal components, which are linear combinations of the original features. These principal components can be treated as the extracted features, and you can choose to use a subset of them to represent the data while reducing dimensionality.\n",
    "\n",
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
    "\n",
    "In the context of building a recommendation system, Min-Max scaling would be used to ensure that the features have similar scales and ranges. This is important because recommendation algorithms often rely on similarity measures, and if features have vastly different scales, one feature might dominate the similarity calculation. Here's how Min-Max scaling could be applied to the dataset:\n",
    "\n",
    "For each feature (price, rating, delivery time):\n",
    "1. Determine the minimum and maximum values of the feature in the dataset.\n",
    "2. Apply the Min-Max scaling formula: scaled_value = (value - min) / (max - min)\n",
    "3. The scaled values will now be in the range [0, 1], making them suitable for feeding into the recommendation algorithm.\n",
    "\n",
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
    "\n",
    "In the context of predicting stock prices, PCA can be used to reduce the dimensionality of the dataset, which may contain a large number of features. Here's how PCA could be applied:\n",
    "\n",
    "1. Standardize the features (mean = 0, variance = 1) to ensure that all features have comparable scales.\n",
    "2. Calculate the covariance matrix of the standardized features.\n",
    "3. Perform eigendecomposition of the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "4. Sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the largest eigenvalues. These eigenvectors are the principal components.\n",
    "5. Project the original data onto the selected principal components to obtain a lower-dimensional representation of the dataset.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "In this example, `num_components` represents the number of principal components that retain at least 95% of the variance in the data. You can adjust the threshold as needed based on your specific requirements and the trade-off between dimensionality reduction and preserving information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a833539",
   "metadata": {},
   "source": [
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28cceb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data: [[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "data_q7 = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "min_value = -1\n",
    "max_value = 1\n",
    "\n",
    "scaled_data_q7 = (data_q7 - np.min(data_q7)) / (np.max(data_q7) - np.min(data_q7)) * (max_value - min_value) + min_value\n",
    "print(\"Scaled data:\", scaled_data_q7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9d59e",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
    "\n",
    "\n",
    "The choice of how many principal components to retain depends on the desired level of dimensionality reduction and the amount of variance you want to preserve. You can look at the explained variance ratio to decide on the number of components to keep. A common approach is to choose a number of components that retains a high percentage of the total variance, such as 95% or 99%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53868cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example data with 5 features and 100 samples\n",
    "data_q8 = np.random.rand(100, 5)\n",
    "\n",
    "pca_q8 = PCA()\n",
    "data_q8_transformed = pca_q8.fit_transform(data_q8)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca_q8.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components that retain at least 95% variance\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "print(\"Number of components to retain:\", num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d490dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
